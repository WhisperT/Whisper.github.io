---
layout: post
title: "EM Method"
date: 2019-05-31 14:25:13
image: '/assets/img/'
description: Brief Introduction of EM Method 
tags:
- Machine learning
categories:
- Bayesian
twitter_text:
---

### EM

EM算法基于概率图模型,其中含有可观测变量 $\mathbf{Y}$,以及不可观测的隐变量 $\mathbf{Z}$. EM算法的最终目的是为了学习得概率图模型中概率分布的参数 
$\theta$. 使用的基本方法是极大似然估计. 但是在这过程中遇到了一些结构性特征. EM就是能把这种隐性的结构性特征利用起来的方法.

### Maximun likelihood Estimation

在一组观测值 $\{y_1,y_,\dots,y_n\}$, 概率图模型可以**假设**这是来源于一个高斯分布$\mathcal{N}(\mu,\sigma^2)$,其中$\mu,\sigma^2$未知,可以根据极大似然估计估计该参数.

$$\begin{align}
L(\mu,\sigma^2|\mathbf{Y})&=\log P(\mathbf{Y}|\mu,\sigma^2)\label{eq:mlegaussian} \\
&=\log \prod_{i=1}^n\mathcal{N}(y_i|\mu,\sigma^2)=(2\pi\sigma^2)^{-\frac{n}{2}}\exp \left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2\right)
\end{align}$$


$$\hat{\mu} = \arg\max L(\mathbf{Y}|\mu,\sigma^2) = \frac{1}{n}\sum_{i=1}^ny_i$$

$$\sigma^2 = \arg\max L(\mathbf{Y}|\hat{\mu},\sigma^2) = \frac{1}{n}\sum_{i=1}^n(y_i-\mu)^2$$

但是当我们知道其中含有结构特征时,就可以更加精确地估计其参数,例如增加参数量.**假设**我们知道这组序列其实来源于两个高斯分布 $\mathcal{N}(\mu_1,\sigma_1^2),\mathcal{N}(\mu_2,\sigma_2^2)$. 此时第$i$个样本是来源于那个高斯分布,这就是结构性特征,这是我们不知道的. 可以假设额外的变量 $\{z_1,z_2,\cdots,z_n\}$. 这就是隐变量. 这里有两种情况,一种情况是$\mathbf{Z}$ 来源于一个总的分布,令外一种情况是每个$z$ 都是来源于各自的分布.